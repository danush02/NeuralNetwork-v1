{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - v1\n",
    "\n",
    "## Description:\n",
    "- This is a simple implementation of a perceptron.\n",
    "- Here, Gradient Descent is used to update the weights.\n",
    "- The activation function used is the sigmoid function.\n",
    "- The loss function used is the sum of squares error.\n",
    "- The dataset used is the Iris dataset(only 2 classes are considered).\n",
    "- This model can be used for dataset having n features and 2 classes.\n",
    "- The dataset is split into training and testing sets.\n",
    "- The accuracy of the model is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # To import data from external file\n",
    "import numpy as np # To store data in multidimensional array format\n",
    "import random # To generate random value for weights and bias initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2:\n",
    "    def __init__(self, max_iter=1000, alpha = 0.001) -> None: # Default values of max iteration and learning rate is set\n",
    "        self.W = 0 # Initializing weights\n",
    "        self.b = np.float32(random.random()) # Initializing Bias\n",
    "        self.alpha = alpha # Setting learning rate value\n",
    "        self.max_iter = max_iter # Setting max iteration value\n",
    "        self.n_features = 0 # Variable to store number of features\n",
    "        self.grad_W = 0 # Initialized gradient value of weights\n",
    "        self.grad_b = 0 # Initialized gradient value of bias\n",
    "\n",
    "    def preactivation(self, X): # Function to calculate the linear equation value\n",
    "        val = np.dot(self.W,X) + self.b\n",
    "        return np.array(val)\n",
    "\n",
    "    def sigmoid(self,val): # Funtion to pass the value obtained from linear equation to activation function i.e. sigmoid\n",
    "        sig_val = np.divide(1,(np.add(1,np.exp(-val)))) # sigmoid = 1 / (1 + exp^(-z))\n",
    "        return sig_val\n",
    "\n",
    "    def model_out(self, sig_val): # Funtion to assign the each sample to respective class based on threshold value 0.5\n",
    "        out = np.where(sig_val > 0.5, 1, 0) # Value from sigmoid if greater than 0.5 sample belongs to class 1, else sample belongs to class 0\n",
    "        return out\n",
    "\n",
    "    def calc_loss(self, y, sig_val): # Function to calculate the loss value\n",
    "        error = np.subtract(sig_val,y) # error = y'(predicted) - y(actual)\n",
    "        loss = np.sum(np.square(error)) # Sum of squared value of error of each sample\n",
    "        return loss\n",
    "\n",
    "    def calc_gradient(self, y, sig_val, X): # Function to calculate the gradient value\n",
    "        error = np.subtract(sig_val,y)\n",
    "        self.grad_W = np.dot(X, error) / len(y) # grad_W = (X.error) / number of samples - this will calculate error effect across the whole sample\n",
    "        self.grad_b = np.mean(error) # Mean value of error\n",
    "\n",
    "    def update_weights(self): # Function to update the weights and bias, here with respect to gradient and learning rate values are changed\n",
    "        self.W -= self.alpha * self.grad_W # W_new = W_old - (learning rate * gradient of weight)\n",
    "        self.b -= self.alpha * self.grad_b # bias_new = bias_old - (learning rate * gradient of bias)\n",
    "\n",
    "    def fit(self, X, y): # Funtion to train the model with respect to training dataset\n",
    "        X = np.array(X).T # Tranpose of feature data\n",
    "        y = np.array(y).T # Transpose of target data\n",
    "        self.n_features = len(X) # Number of features\n",
    "        self.W = np.random.rand(self.n_features) # Weigts initialized based on number of features using random values\n",
    "        self.grad_W = np.zeros_like(self.W) # Gradient of weights initialized with zeros based on the number of weights\n",
    "        for i in range(self.max_iter+1): # Loop to iterate with respect to max iteration value\n",
    "            sig_val = self.sigmoid(self.preactivation(X)) # Calculate the value from activation function\n",
    "            loss_val = self.calc_loss(y,sig_val) # Calculate the loss value\n",
    "            if i%10000 == 0: # For each 10,000 iteration the loss value, weights and gradients are displayed\n",
    "                print(f\"Loss Value: {np.mean(loss_val)}, Iteration: {i}\")\n",
    "                print(f\"Weights: {self.W}\")\n",
    "                print(f\"Gradient: {self.grad_W}, Bias: {self.grad_b}\")\n",
    "            self.calc_gradient(y, sig_val, X) # Calculate the gradient\n",
    "            self.update_weights() # Update the weight and bias\n",
    "\n",
    "    def predict(self, X): # Funtion to predict the class of a sample\n",
    "        X = np.array(X).T\n",
    "        return self.model_out(self.sigmoid(self.preactivation(X))) # Class of each sample is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm:\n",
    "1. Initialize the weights and bias with random values.\n",
    "2. Calculate the weighted sum of inputs.\n",
    "3. Apply the activation function to the weighted sum.\n",
    "4. Calculate the error.\n",
    "5. Calculate the gradient of the error with respect to the weights.\n",
    "6. Update the weights using the gradient descent algorithm.\n",
    "7. Repeat steps 2-6 until the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/Learn ML/Dataset/Iris.csv\")\n",
    "target_map = {\"Iris-setosa\":0, \"Iris-versicolor\":1}\n",
    "df['Species'] = df['Species'].map(target_map)\n",
    "df.head()\n",
    "df.drop(\"Id\",axis=1)\n",
    "nn = NeuralNetwork2(max_iter=60000,alpha=0.05)\n",
    "taget_col = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "feature_df = df[taget_col]\n",
    "target_df = df['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model and Calculating the Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Value: 8.950988422883496, Iteration: 0\n",
      "Weights: [0.77122386 0.33442314 0.32362175 0.36584116]\n",
      "Gradient: [0. 0. 0. 0.], Bias: -1.5375746942095396e-06\n",
      "Loss Value: 0.001317959179533473, Iteration: 10000\n",
      "Weights: [ 0.11317407 -2.26102506  4.089811    2.0959847 ]\n",
      "Gradient: [ 0.0002208   0.00096019 -0.00143452 -0.00071985], Bias: 0.0001118514607231492\n",
      "Loss Value: 0.00045028796145136986, Iteration: 20000\n",
      "Weights: [ 0.03473791 -2.59727221  4.59423286  2.35211042]\n",
      "Gradient: [ 0.00011614  0.00048996 -0.00073841 -0.0003792 ], Bias: 5.875309239457724e-05\n",
      "Loss Value: 0.00023785170571618736, Iteration: 30000\n",
      "Weights: [-0.01314477 -2.79699864  4.89621011  2.50817284]\n",
      "Gradient: [ 8.00825196e-05  3.30101341e-04 -5.00802195e-04 -2.60413136e-04], Bias: 4.0508878981539795e-05\n",
      "Loss Value: 0.0001505339252834042, Iteration: 40000\n",
      "Weights: [-0.04814293 -2.93993476  5.1136407   2.62171319]\n",
      "Gradient: [ 6.16150284e-05  2.49262587e-04 -3.80220626e-04 -1.99367843e-04], Bias: 3.117431754212825e-05\n",
      "Loss Value: 0.00010528799319192642, Iteration: 50000\n",
      "Weights: [-0.07592491 -3.05144955  5.28413345  2.71138883]\n",
      "Gradient: [ 5.03205490e-05  2.00378136e-04 -3.07074004e-04 -1.62007208e-04], Bias: 2.5467099251716294e-05\n",
      "Loss Value: 7.848350490205849e-05, Iteration: 60000\n",
      "Weights: [-0.09906063 -3.14295168  5.42464075  2.78569833]\n",
      "Gradient: [ 4.26688617e-05  1.67595813e-04 -2.57882560e-04 -1.36711109e-04], Bias: 2.1600804823621772e-05\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df, test_size=0.3, random_state=42)\n",
    "nn.fit(X=X_train,y=y_train)\n",
    "y_out = nn.predict(X=X_test)\n",
    "print(accuracy_score(y_test,y_out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
